// FP8 GEMM kernel template for H100 architecture
// Template parameters will be replaced at runtime:
// {{BLOCK_SIZE_M}}, {{BLOCK_SIZE_N}}, {{BLOCK_SIZE_K}}
// {{UNROLL_FACTOR}}

extern "C" __global__ void fp8_gemm_kernel(
    const __nv_fp8_e4m3fn* A,
    const __nv_fp8_e4m3fn* B,
    float* C,
    size_t m, size_t n, size_t k,
    size_t lda, size_t ldb, size_t ldc,
    bool trans_a, bool trans_b,
    float scale_A, float scale_B, float scale_C,
    float alpha, float beta) {

    // Block index
    const int block_row = blockIdx.y;
    const int block_col = blockIdx.x;

    // Thread index
    const int thread_row = threadIdx.y;
    const int thread_col = threadIdx.x;

    // Shared memory for A and B blocks
    __shared__ __nv_fp8_e4m3fn shared_A[{{BLOCK_SIZE_M}}][{{BLOCK_SIZE_K}}];
    __shared__ __nv_fp8_e4m3fn shared_B[{{BLOCK_SIZE_K}}][{{BLOCK_SIZE_N}}];

    // Calculate global row and column indices for C matrix
    const int row = block_row * {{BLOCK_SIZE_M}} + thread_row;
    const int col = block_col * {{BLOCK_SIZE_N}} + thread_col;

    // Accumulator register for this thread
    float acc = 0.0f;

    // Loop over k-dimension in blocks
    for (int bk = 0; bk < (k + {{BLOCK_SIZE_K}} - 1) / {{BLOCK_SIZE_K}}; ++bk) {
        // Load A block into shared memory
        #pragma unroll
        for (int i = 0; i < {{BLOCK_SIZE_M}}; i += {{UNROLL_FACTOR}}) {
            const int a_row = trans_a ?
                (bk * {{BLOCK_SIZE_K}} + thread_col) :
                (block_row * {{BLOCK_SIZE_M}} + i + thread_row);
            const int a_col = trans_a ?
                (block_row * {{BLOCK_SIZE_M}} + i + thread_row) :
                (bk * {{BLOCK_SIZE_K}} + thread_col);

            if (a_row < m && a_col < k) {
                shared_A[i + thread_row][thread_col] = trans_a ?
                    A[a_col * lda + a_row] :
                    A[a_row * lda + a_col];
            } else {
                shared_A[i + thread_row][thread_col] = __float_to_nv_fp8_e4m3fn(0.0f);
            }
        }

        // Load B block into shared memory
        #pragma unroll
        for (int i = 0; i < {{BLOCK_SIZE_N}}; i += {{UNROLL_FACTOR}}) {
            const int b_row = trans_b ?
                (col + i) :
                (bk * {{BLOCK_SIZE_K}} + thread_row);
            const int b_col = trans_b ?
                (bk * {{BLOCK_SIZE_K}} + thread_row) :
                (col + i);

            if (b_row < k && b_col < n) {
                shared_B[thread_row][i + thread_col] = trans_b ?
                    B[b_col * ldb + b_row] :
                    B[b_row * ldb + b_col];
            } else {
                shared_B[thread_row][i + thread_col] = __float_to_nv_fp8_e4m3fn(0.0f);
            }
        }

        __syncthreads();

        // Perform matrix multiplication for this block
        #pragma unroll {{UNROLL_FACTOR}}
        for (int k_idx = 0; k_idx < {{BLOCK_SIZE_K}}; ++k_idx) {
            // Convert fp8 values to float and accumulate
            float a_val = __nv_fp8_e4m3fn_to_float(shared_A[thread_row][k_idx]) / scale_A;
            float b_val = __nv_fp8_e4m3fn_to_float(shared_B[k_idx][thread_col]) / scale_B;
            acc += a_val * b_val;
        }

        __syncthreads();
    }

    // Store result to global memory if within bounds
    if (row < m && col < n) {
        if (beta == 0.0f) {
            C[row * ldc + col] = alpha * acc * scale_C;
        } else {
            C[row * ldc + col] = alpha * acc * scale_C + beta * C[row * ldc + col];
        }
    }
}
